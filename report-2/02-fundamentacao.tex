\section{Fundamentação Teórica}
\subsection{Banco de Dados Distribuído}
Um banco de dados distribuído é um banco de dados cujos dados estão armazenados em diferentes locais, mas que se intercomunicam de forma a parecerem um único banco de dados para o usuário.

A distribuição dos dados entre várias máquinas possibilita melhorar a escalabilidade, a disponibilidade e a tolerância a falhas do sistema. Entretanto, essa distribuição implica em custos adicionais de comunicação e sincronização entre os nós do sistema.

No contexto da distribuição dos dados em bancos de dados distribuídos, tanto as tuplas quanto os atributos que as compõem podem ser fragmentados entre os nós do sistema, e cópias desses dados podem ser replicadas em mais de um nó.

%\subsubsection{Fragmentação Horizontal}
%A fragmentação horizontal consiste na distribuição das tuplas inteiras de uma tabela entre diferentes máquinas do sistema. Essa fragmentação é realizada de modo que cada fragmento contenha um subconjunto das tuplas da tabela original, mantendo todas as colunas.

%\subsubsection{Fragmentação Vertical}
%A fragmentação vertical consiste na distribuição dos atributos de uma tabela entre diferentes máquinas do sistema, mantendo em comum a chave primária da tabela original. Dessa forma, cada fragmento armazena um subconjunto dos atributos para cada tupla da tabela original.

%\subsubsection{Fragmentação Mista}
%A fragmentação mista combina as estratégias de fragmentação horizontal e vertical. Nesse caso, cada fragmento armazena um subconjunto dos atributos de uma tabela para um subconjunto das tuplas.

\subsubsection{Replicação}
No que diz respeito à redundância dos dados entre os nós do sistema, a replicação consiste na cópia de um fragmento de dados em mais de um nó. Essa cópia pode ser feita de forma parcial ou completa, e uma maior redundância dos dados implica em maior disponibilidade e tolerância a falhas do sistema, mas aumenta os custos de armazenamento e sincronização.

%\subsubsection{Homogeneidade e Heterogeneidade}
%Os bancos de dados distribuídos também podem ser compostos por nós de mesma natureza, no que diz respeito ao SGBD utilizado, modelos de dados, protocolos de comunicação, fragmentação e replicação dos dados.

Um sistema cujos nós seguem a mesma arquitetura geral é classificado como homogêneo, e apresenta maior desempenho geral, simplicidade de implementação e escalabilidade.

Por outro lado, um sistema heterogêneo é composto por nós de diferentes arquiteturas, o que aumenta a complexidade de implementação e manutenção, além de exigir um esforço maior de intercomunicação entre os nós para garantir a transparência ao usuário.

\subsubsection{Alocação de Dados}
A alocação de dados em um banco de dados distribuído é o processo pelo qual se decide onde os dados serão armazenados, levando em consideração a configuração dos nós do sistema e as métricas de desempenho desejadas. O processo de alocação de dados pode ser feito de forma centralizada ou descentralizada.

%\subsubsection{Consulta em Banco de Dados Distribuído}
%Devido à variedade de configurações e aos fragmentos de dados contidos nos nós do sistema, o processo de consulta se torna mais complexo em um banco de dados distribuído.

%\subsubsection{Plano de Consulta}
%O plano de consulta em bancos de dados distribuídos é a sequência de operações que define como uma consulta será executada sobre os dados fragmentados e possivelmente replicados entre diferentes nós. O otimizador do SGBD distribui as operações entre os nós, buscando minimizar o custo de comunicação e o tempo de resposta. O plano pode envolver o envio de subconsultas para diferentes nós, a coleta e combinação dos resultados parciais, e a aplicação de operações finais no nó coordenador. A eficiência do plano de consulta é fundamental para o desempenho do sistema distribuído.

\subsubsection{Transações}
Transações em bases de dados distribuídas envolvem múltiplos nós ou locais onde os dados estão armazenados, mas precisam ser executadas de forma coordenada para garantir as propriedades de Atomicidade, Consistência, Isolamento e Durabilidade (ACID). Mesmo que uma transação envolva atualizações em diferentes servidores, ela deve ser concluída integralmente em todos eles ou ser completamente desfeita em caso de falha, mantendo o sistema em um estado consistente.
Para garantir essas propriedades, sistemas distribuídos utilizam protocolos especiais, como o protocolo de commit em duas fases (Two-Phase Commit – 2PC), que coordenam a aprovação e execução das operações entre todos os nós participantes. Esses mecanismos são essenciais para evitar inconsistências e garantir a confiabilidade das transações./


\subsection{Apache Cassandra}
Para o desenvolvimento do projeto, foi utilizado o Apache Cassandra, um banco de dados NoSQL distribuído.

\subsubsection{Estrutura de Dados e Particionamento}

Os dados no Cassandra são organizados em keyspaces (equivalentes a bancos de dados),
que agrupam tabelas relacionadas. Cada tabela é composta por linhas e colunas,
sendo que cada linha é identificada por uma chave primária composta por um partition key e, opcionalmente, colunas de ordenação (clustering columns).
O partition key é fundamental para a distribuição dos dados: ele é transformado em um token por meio de um particionador, e esse token determina em qual nó do cluster a linha será armazenada.

A distribuição dos dados é feita de forma que cada nó do cluster seja responsável por um intervalo do espaço de tokens.
Quando novos nós são adicionados, o espaço de tokens é redistribuído automaticamente, promovendo o balanceamento de carga e a escalabilidade linear do sistema.


\subsubsection{Replicação e Tolerância a Falhas}

Cassandra implementa replicação configurável por keyspace, permitindo definir o fator de replicação (quantidade de cópias de cada dado) e a
estratégia de replicação mais adequada ao ambiente. 
O mecanismo de replicação, junto à arquitetura descentralizada, garante alta disponibilidade e recuperação automática de falhas,
com mecanismos como hinted handoff para garantir consistência e reparo automático de dados entre nós.

\subsubsection{Consistência e Transações}

Um dos grandes diferenciais do Cassandra é a consistência ajustável (tunable consistency), sendo que o usuário pode definir,
para cada operação, quantos nós precisam confirmar uma leitura ou escrita para que ela seja considerada bem-sucedida.

Isso permite ajustar entre priorizar consistência forte ou disponibilidade, conforme a necessidade da aplicação.
Por padrão, o que diz respeito ao teorema CAP, o Cassandra opera por padrão como um sistema AP (alta disponibilidade e tolerância a partições), mas pode ser configurado para 
comportar-se como CP(consistência e tolerância a partições) em cenários específicos.

Cassandra não implementa transações distribuídas entre múltiplas partições,
mas oferece suporte a transações leves (lightweight transactions) com isolamento serial em nível de partição.

\subsubsection{Escalabilidade}

Cassandra é reconhecido pela adição ou remoção de nós poder ser feita sem downtime,
e o throughput de leitura e escrita cresce linearmente com o número de nós.
Isso o torna ideal para aplicações que exigem alta disponibilidade, tolerância a falhas e capacidade de lidar com grandes volumes de dados distribuídos globalmente.